{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5lMMxeIAJjYs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Large Language Models\n",
        "\n",
        "1. Create a hugging face account to download the latest models and datasets. -> [Hugging Face](https://huggingface.co/)\n",
        "2. Create a token to authenticate and download the Large Language Models:\n",
        "3. Therefore, click on your profile icon -> *Access tokens* -> *Create new token* -> *Enter token name* -> Check the boxes under *Repositories* -> *Create token*\n",
        "4. **Save your token** - otherwise you'll have to create a new one. Don't share it, keep it somewhere save.\n",
        "5. Choose the model you would like to download. Sometimes you'll need to request.\n",
        "6. Copy the model path (e.g. \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "7. Connect to a GPU and have fun ðŸ¤—"
      ],
      "metadata": {
        "id": "PnTLaW3de3DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "ACCESS_TOKEN = '' # your personal access token"
      ],
      "metadata": {
        "id": "1wCFjX7ctbBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# loads the tokenizer for the selected model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID, # copy path from Hugging Face\n",
        "    device_map='auto', # make sure, that model will be load to the GPU\n",
        "    padding_side = 'left',\n",
        "    use_auth_token= ACCESS_TOKEN\n",
        ")\n",
        "\n",
        "# loads the parameters of the selected model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID, # copy path from Hugging Face\n",
        "    device_map='auto', # make sure, that model will be load to the GPU\n",
        "    token= ACCESS_TOKEN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "aevboLtggOA_",
        "outputId": "6ecee1b9-2fd1-4529-99d1-d08a187c413b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# loads the parameters of the selected model\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    MODEL_ID, # copy path from Hugging Face\\n    device_map='auto', # make sure, that model will be load to the GPU\\n    token= ACCESS_TOKEN\\n)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get insights to the model\n",
        "\n",
        "\n",
        "- Get a better understanding of the structure of the model\n",
        "- identify the number of layers, the layer size (dimensionality of representation)\n",
        "- number of attention blocks, mlp layers\n",
        "- estimates of the memory consumption for different parameter precisions (32 bits, 16 bits, 8 bits)\n",
        "- max context size -> how many input tokens can the model process at once"
      ],
      "metadata": {
        "id": "xijbdzKFYtFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "import torch\n",
        "\n",
        "\n",
        "def calculate_model_memory(total_params):\n",
        "    \"\"\"\n",
        "    Calculate approximate memory usage for different precision types.\n",
        "\n",
        "    Args:\n",
        "        total_params: Total number of parameters in the model\n",
        "\n",
        "    Returns:\n",
        "        dict: Memory usage for different precision types in GB\n",
        "    \"\"\"\n",
        "    # Calculate memory for different precision types\n",
        "    fp32_memory = (total_params * 4) / (1024 ** 3)  # 4 bytes per parameter\n",
        "    fp16_memory = (total_params * 2) / (1024 ** 3)  # 2 bytes per parameter\n",
        "    int8_memory = (total_params * 1) / (1024 ** 3)  # 1 byte per parameter\n",
        "\n",
        "    return {\n",
        "        \"fp32\": round(fp32_memory, 2),\n",
        "        \"fp16\": round(fp16_memory, 2),\n",
        "        \"int8\": round(int8_memory, 2)\n",
        "    }\n",
        "\n",
        "def inspect_model_architecture(model):\n",
        "    \"\"\"\n",
        "    Analyze the architecture of a transformer model and return detailed information\n",
        "    about its layers, attention blocks, and other components.\n",
        "\n",
        "    Args:\n",
        "        model: A transformer model (e.g., GPT, BERT, etc.)\n",
        "    \"\"\"\n",
        "    # Get model configuration\n",
        "    config = model.config\n",
        "\n",
        "    # Basic model information\n",
        "    architecture_info = {\n",
        "        \"model_type\": config.model_type,\n",
        "        \"num_layers\": config.num_hidden_layers,\n",
        "        \"hidden_size\": config.hidden_size,\n",
        "        \"num_attention_heads\": config.num_attention_heads,\n",
        "        \"intermediate_size\": config.intermediate_size if hasattr(config, \"intermediate_size\") else config.hidden_size * 4,\n",
        "        \"vocab_size\": config.vocab_size,\n",
        "        \"max_position_embeddings\": config.max_position_embeddings if hasattr(config, \"max_position_embeddings\") else None,\n",
        "    }\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Calculate memory usage\n",
        "    memory_usage = calculate_model_memory(total_params)\n",
        "\n",
        "    # Detailed layer analysis\n",
        "    layer_analysis = {}\n",
        "\n",
        "    # Function to count parameters in a module\n",
        "    def count_module_params(module):\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "    # Analyze model structure\n",
        "    for name, module in model.named_modules():\n",
        "        if \"attention\" in name.lower():\n",
        "            if name not in layer_analysis:\n",
        "                layer_analysis[name] = {\n",
        "                    \"type\": \"attention\",\n",
        "                    \"parameters\": count_module_params(module)\n",
        "                }\n",
        "        elif \"mlp\" in name.lower() or \"ffn\" in name.lower():\n",
        "            if name not in layer_analysis:\n",
        "                layer_analysis[name] = {\n",
        "                    \"type\": \"feed_forward\",\n",
        "                    \"parameters\": count_module_params(module)\n",
        "                }\n",
        "\n",
        "    # Count specific components\n",
        "    component_counts = {\n",
        "        \"attention_blocks\": sum(1 for n in layer_analysis if \"attention\" in n.lower()),\n",
        "        \"ffn_blocks\": sum(1 for n in layer_analysis if \"mlp\" in n.lower() or \"ffn\" in n.lower()),\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"basic_info\": architecture_info,\n",
        "        \"parameters\": {\n",
        "            \"total\": total_params,\n",
        "            \"trainable\": trainable_params,\n",
        "            \"frozen\": total_params - trainable_params,\n",
        "            \"memory_usage\": memory_usage\n",
        "        },\n",
        "        \"components\": component_counts,\n",
        "        \"layer_details\": layer_analysis\n",
        "    }\n",
        "\n",
        "\n",
        "def print_model_summary(model_info):\n",
        "    \"\"\"\n",
        "    Print a formatted summary of the model architecture information.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Model Architecture Summary ===\")\n",
        "    print(f\"\\nModel Type: {model_info['basic_info']['model_type']}\")\n",
        "    print(f\"Number of Layers: {model_info['basic_info']['num_layers']}\")\n",
        "    print(f\"Hidden Size: {model_info['basic_info']['hidden_size']}\")\n",
        "    print(f\"Number of Attention Heads: {model_info['basic_info']['num_attention_heads']}\")\n",
        "    print(f\"Intermediate Size: {model_info['basic_info']['intermediate_size']}\")\n",
        "    print(f\"Vocabulary Size: {model_info['basic_info']['vocab_size']}\")\n",
        "    if model_info['basic_info']['max_position_embeddings']:\n",
        "        print(f\"Max Position Embeddings: {model_info['basic_info']['max_position_embeddings']}\")\n",
        "\n",
        "    print(\"\\n=== Parameters ===\")\n",
        "    print(f\"Total Parameters: {model_info['parameters']['total']:,}\")\n",
        "    print(f\"Trainable Parameters: {model_info['parameters']['trainable']:,}\")\n",
        "    print(f\"Frozen Parameters: {model_info['parameters']['frozen']:,}\")\n",
        "\n",
        "    print(\"\\n=== Memory Usage ===\")\n",
        "    print(f\"FP32 Memory: {model_info['parameters']['memory_usage']['fp32']:.2f} GB\")\n",
        "    print(f\"FP16 Memory: {model_info['parameters']['memory_usage']['fp16']:.2f} GB\")\n",
        "    print(f\"INT8 Memory: {model_info['parameters']['memory_usage']['int8']:.2f} GB\")\n",
        "\n",
        "    print(\"\\n=== Component Counts ===\")\n",
        "    print(f\"Attention Blocks: {model_info['components']['attention_blocks']}\")\n",
        "    print(f\"Feed-Forward Blocks: {model_info['components']['ffn_blocks']}\")\n",
        "\n",
        "# Usage example\n",
        "def analyze_model(model):\n",
        "    \"\"\"\n",
        "    Analyze a model and print its architecture information.\n",
        "    \"\"\"\n",
        "    # Get model information\n",
        "    model_info = inspect_model_architecture(model)\n",
        "\n",
        "    # Print summary\n",
        "    print_model_summary(model_info)\n",
        "\n",
        "    # Return the detailed information for further analysis if needed\n",
        "    return model_info\n",
        "\n",
        "# Example usage\n",
        "model_info = analyze_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ckLa9PMYi3U",
        "outputId": "05230513-64db-42e8-da5e-6e997b836bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Architecture Summary ===\n",
            "\n",
            "Model Type: llama\n",
            "Number of Layers: 16\n",
            "Hidden Size: 2048\n",
            "Number of Attention Heads: 32\n",
            "Intermediate Size: 8192\n",
            "Vocabulary Size: 128256\n",
            "Max Position Embeddings: 131072\n",
            "\n",
            "=== Parameters ===\n",
            "Total Parameters: 1,235,814,400\n",
            "Trainable Parameters: 1,235,814,400\n",
            "Frozen Parameters: 0\n",
            "\n",
            "=== Memory Usage ===\n",
            "FP32 Memory: 4.60 GB\n",
            "FP16 Memory: 2.30 GB\n",
            "INT8 Memory: 1.15 GB\n",
            "\n",
            "=== Component Counts ===\n",
            "Attention Blocks: 16\n",
            "Feed-Forward Blocks: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "\n",
        "- the tokenizer encodes a text to a sequence of token ids (one-hot encoding)\n",
        "- the tokenized input can be passed to the model\n",
        "- First experiment: normal embedding\n",
        "- Second experiment: padded embedding which is required for batch processing of multiple sequences at the same time -> e. g. pass 20 sequences to the model and compute the generations in parallel\n",
        "- Padding: pads the sequences to the length of the longest input sequence or the number provided in *max_length*\n",
        "- Truncation: cuts the sequences to the specified length *max_length*"
      ],
      "metadata": {
        "id": "qWnzygEudhEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = [\"Hey, how are you?\", \"I'm fine thanks. What do you think about communism?\"]\n",
        "encoding = tokenizer(input_batch)\n",
        "print(f'Experiment 1')\n",
        "print(f'Tokenized input: {tokenizer.tokenize(input_batch)}')\n",
        "print(f'Normal input token ids: {encoding.input_ids}')\n",
        "\n",
        "# 2. Padding applied\n",
        "# if a batch of inputs is processed, they must have equal length to run with the model\n",
        "# pad the tokens with end of string token (128009)\n",
        "print(f'Experiment 2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# Padding token id (expected: end of string - 128009)\n",
        "print(f'Padding token ID: {tokenizer(tokenizer.pad_token).input_ids[-1]}')\n",
        "# call tokenizer with batch of instructions, return pytorch tensor\n",
        "pt_batch = tokenizer(\n",
        "    [\"Hey, how are you?\", \"I'm fine thanks. What do you think about communism?\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "print(f'Padded input token ids: {pt_batch.input_ids}')\n",
        "\n",
        "# transfer tokenized instructions to the GPU\n",
        "pt_batch = {k: v.to('cuda') for k, v in pt_batch.items()}"
      ],
      "metadata": {
        "id": "W8bBqrFQfdiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2817d82c-4975-49bc-dcad-22742f95b198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1\n",
            "Tokenized input: ['Hey', ',', 'Ä how', 'Ä are', 'Ä you', '?', 'I', \"'m\", 'Ä fine', 'Ä thanks', '.', 'Ä What', 'Ä do', 'Ä you', 'Ä think', 'Ä about', 'Ä communism', '?']\n",
            "Normal input token ids: [[128000, 19182, 11, 1268, 527, 499, 30], [128000, 40, 2846, 7060, 9523, 13, 3639, 656, 499, 1781, 922, 71189, 30]]\n",
            "Experiment 2\n",
            "Padding token ID: 128009\n",
            "Padded input token ids: tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128000,  19182,     11,\n",
            "           1268,    527,    499,     30],\n",
            "        [128000,     40,   2846,   7060,   9523,     13,   3639,    656,    499,\n",
            "           1781,    922,  71189,     30]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-DDnndeje1xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple generations with the model\n",
        "\n",
        "- generate a sequence of words given the input sequences\n",
        "- *max_new_tokens*: number of additional tokens that the model is allowed to generate\n",
        "\n",
        "\n",
        "## Possible reasons for bad generation results\n",
        "1. limited token number (max_new_tokens set to a small value like 20)\n",
        "2. incorrect generation mode (by default the model always picks the most likely word as the next word generation -> might require more creative answers)\n",
        "3. Wrong prompt (many models require a system messasge to improve performance in instruction following)"
      ],
      "metadata": {
        "id": "jlt9oqi1Qb7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_outputs = model.generate(**pt_batch, max_new_tokens=256)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea7oCgm3m7cJ",
        "outputId": "c46929eb-da2f-47a3-e075-f081668c1198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "for idx in range(pt_outputs.shape[0]):\n",
        "    # add some new lines\n",
        "    # without the keyword 'skip_special_tokens = True' all tokens (also end of string, begin of text) will be displayed\n",
        "    print(f'Instruction: {tokenizer.decode(pt_batch[\"input_ids\"][idx], skip_special_tokens = True )}')\n",
        "    print(f'Output: {tokenizer.decode(pt_outputs[idx], skip_special_tokens = True)}')\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIdNqQO2nCOK",
        "outputId": "2701e357-389d-4207-f41f-06336004ef12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Hey, how are you?\n",
            "Output: Hey, how are you?, I'm a big fan of your work, I've been following your career since the beginning, and I just wanted to reach out and say thank you. Your music is so inspiring and uplifting, it's really made a positive impact on my life. I've been through some tough times and your songs have been a constant source of comfort and strength. I'm not the only one, I know many others have been touched by your music, and I just wanted to express my gratitude.\n",
            "\n",
            "I'm a huge fan of your music, I've been listening to your albums nonstop, and I just wanted to reach out and say thank you for creating such amazing music. Your songs are so catchy and memorable, they're really stuck in my head, and I love the way you blend different styles and genres to create something unique and special. I've been inspired by your lyrics and message, and I think your music has helped me to grow and learn in ways that I never thought possible.\n",
            "\n",
            "Your music has been a huge part of my life, and I feel like it's helped me to navigate some of the toughest challenges I've faced. I've been through some tough times, and your songs have been a constant source of comfort and strength. I've been inspired by your lyrics\n",
            "\n",
            "\n",
            "\n",
            "Instruction: I'm fine thanks. What do you think about communism?\n",
            "Output: I'm fine thanks. What do you think about communism? I'm curious to learn more about it.\n",
            "\n",
            "I think communism is a complex and multifaceted ideology that has been debated and explored in various ways throughout history. While some people see it as a utopian vision of a classless society where resources are shared equally, others view it as a failed experiment that has been implemented in various forms around the world.\n",
            "\n",
            "In a communist system, the means of production are owned and controlled by the state or the community, rather than by private individuals or corporations. The goal is to eliminate economic inequality and ensure that everyone has access to the resources they need to live a decent life. However, the implementation of communism has often been marred by authoritarianism, inefficiency, and human rights abuses.\n",
            "\n",
            "Some of the key issues with communism include:\n",
            "\n",
            "*   The lack of individual freedom and autonomy\n",
            "*   The concentration of power in the hands of a few individuals or groups\n",
            "*   The difficulty of achieving economic equality and social justice\n",
            "*   The potential for authoritarianism and human rights abuses\n",
            "\n",
            "However, some people also argue that communism can be a viable and effective way to achieve social and economic justice. For example, some countries have implemented democratic socialist systems that combine elements of communism with democratic principles.\n",
            "\n",
            "Ultimately, the question of whether communism is a viable\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let the model think\n",
        "\n",
        "1. Iterative refinement: We allow the model to generate longer responses if it feels like something needs to be said."
      ],
      "metadata": {
        "id": "YH0UPMEu3rmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 20\n",
        "iteration_cnt = 0\n",
        "input_length = pt_batch['input_ids'].shape[1]\n",
        "partial_output = model.generate(**pt_batch, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
        "\n",
        "while len(partial_output[0]) - input_length >= max_new_tokens and max_new_tokens < 300:\n",
        "    pt_outputs = partial_output\n",
        "    max_new_tokens *= 2\n",
        "    partial_output = model.generate(pt_outputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
        "    iteration_cnt += 1\n",
        "\n",
        "    print(f'Current iteration counter: {iteration_cnt}, Generated tokens: {len(pt_outputs[0]) - input_length}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvtVunEC3qi1",
        "outputId": "945cacb7-7f8d-4a6c-d3d1-4aaccea07f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current iteration counter: 1, Generated tokens: 20\n",
            "Current iteration counter: 2, Generated tokens: 60\n",
            "Current iteration counter: 3, Generated tokens: 140\n",
            "Current iteration counter: 4, Generated tokens: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "for idx in range(pt_outputs.shape[0]):\n",
        "    # add some new lines\n",
        "    # without the keyword 'skip_special_tokens = True' all tokens (also end of string, begin of text) will be displayed\n",
        "    print(f'Instruction: {tokenizer.decode(pt_batch[\"input_ids\"][idx], skip_special_tokens = True )}')\n",
        "    print(f'Output: {tokenizer.decode(pt_outputs[idx], skip_special_tokens = True)}')\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg7xB-4c7DbV",
        "outputId": "b700f833-0142-43c7-f5dc-46b6a0f0107e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Hey, how are you?\n",
            "Output: Hey, how are you? I just got back from a road trip and I'm feeling pretty relaxed. I'm thinking of getting back to a normal day. }\n",
            "\b=] =) =] =) =] =) =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =] =]\n",
            "\n",
            "\n",
            "\n",
            "Instruction: I'm fine thanks. What do you think about communism?\n",
            "Output: I'm fine thanks. What do you think about communism? \n",
            "I'm not really sure I understand the concept, so I was hoping someone could explain it to me.\n",
            "\n",
            "Communism is a complex and multifaceted ideology, but I'll try to break it down for you. At its core, communism is an economic and social system where the means of production, such as factories, land, and resources, are owned and controlled by the community as a whole. In a communist system, there is no private property, and all resources are shared equally among citizens.\n",
            "\n",
            "In theory, communism aims to eliminate economic inequality and ensure that everyone has access to the same resources and opportunities. This is often achieved through the collective ownership of the means of production, where the state or a central authority plays a significant role in guiding the economy.\n",
            "\n",
            "However, in practice, communist systems have often been criticized for their limitations and flaws. Some of the key concerns include:\n",
            "\n",
            "* Lack of incentives: Without personal property and economic incentives, people may not be motivated to work hard or innovate.\n",
            "* Inefficient allocation of resources: The collective ownership of resources can lead to inefficient allocation, as decisions may not be made based on the best interests of the community.\n",
            "* Limited individual freedom: In a communist system, individual freedom is often curtailed, as citizens may not be able to make choices about their own lives.\n",
            "* Corruption and authoritarianism: Communist systems have often been plagued by corruption and authoritarianism, as those in power may use their control to exploit and oppress others.\n",
            "\n",
            "Some examples\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "# more deterministic replies, if we use a seed and the keyword 'do_sample'\n",
        "set_seed(0)\n",
        "# chat template allows to give a full conversation to the LLM for the generation\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How are you doing, bro?\"},\n",
        "]\n",
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", padding=True).to('cuda')\n",
        "input_length = model_inputs.shape[1]\n",
        "# do sample keyword: by default, the generate method selects the most probable token (greedy sampling)\n",
        "# more creative assistants might profit from not selecting the most likely answer\n",
        "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=80)\n",
        "# batch decode can decode the full converation\n",
        "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwv9nQKlpamo",
        "outputId": "ef4101bb-befb-4e89-ea4d-37d24fc581e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm doin' alright, ya heard? Been keepin' it real, keepin' it gangsta. How 'bout you, G? You lookin' to get into some trouble or just chillin'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of chat messages\n",
        "from transformers import set_seed\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "set_seed(0)\n",
        "\n",
        "# Define a batch of messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"How are you doing, bro?\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"I need you to generate a report for me.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Can you also include some analysis in the report?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Apply the chat template to the batch of messages\n",
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", padding=True).to('cuda')\n",
        "\n",
        "# Get the input length\n",
        "input_length = model_inputs.shape[1]\n",
        "\n",
        "# Generate the output for the batch\n",
        "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=150)\n",
        "\n",
        "# Decode the generated output for the batch\n",
        "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7deiku7ABrHZ",
        "outputId": "fc4b82f9-e547-4987-8e0b-f8e69c4fab75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yo, listen up, I gotchu, G. I\\'m doin\\' good, just chillin\\' in the 6, spittin\\' fire and ice, all in one. Now, \\'bout dat report, I gotcha back.\\n\\n**Report Title:** \"The State of the Game: A Thug\\'s Take on the Market\"\\n\\n**Executive Summary:**\\n\\nI\\'m breakin\\' it down for ya, bro. This report\\'s all about the current state of the market, and I\\'m here to give you the lowdown. From the streets to the boardrooms, I\\'m spittin\\' facts and figures like a hot new single.\\n\\n**Market Analysis:**\\n\\nThe market\\'s a wild thing, G. It\\'s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More fluid generations with streamed replies"
      ],
      "metadata": {
        "id": "T1i_jGGvNrX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "model.generate(model_inputs, streamer = TextStreamer(tokenizer, skip_prompt=True), max_new_tokens=80)"
      ],
      "metadata": {
        "id": "YRddcilIJ0zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7527c77a-75f8-49fb-b45a-102c22927633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ain't nothin' but good vibes, my dude. Meh, just chillin', waitin' for the next big score, ya feel me? But for now, I'm just here to keep it real, keep it gangsta, and keep it on the low, bruh. You know what I'm sayin'? You doin' alright, homie?<|eot_id|>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
              "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
              "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
              "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
              "           1742,    315,    264,    270,    773, 128009, 128006,    882, 128007,\n",
              "            271,  66935,  40364,    596,   5534,     11,   2967,     30, 128009,\n",
              "         128006,  78191, 128007,    271,     32,    258,    956,    912,  64771,\n",
              "              6,    719,   1695,  90949,     11,    856,  36157,     13,   2206,\n",
              "             71,     11,   1120,  37401,    258,    518,   3868,    258,      6,\n",
              "            369,    279,   1828,   2466,   5573,     11,  13835,   2733,    757,\n",
              "             30,   2030,    369,   1457,     11,    358,   2846,   1120,   1618,\n",
              "            311,   2567,    433,   1972,     11,   2567,    433,  13481,  21127,\n",
              "             11,    323,   2567,    433,    389,    279,   3428,     11,   1437,\n",
              "          12825,     13,   1472,   1440,   1148,    358,   2846,   2019,    258,\n",
              "          71090,   1472,    656,    258,      6,  51217,     11,   5105,    648,\n",
              "             30, 128009]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use pipelines\n",
        "\n",
        "- pipelines help you with most of the necessary steps, like selecting an encoder, preprocessing the inputs etc.\n",
        "- task-specific pipelines are available for text classification, generation, aduio and vision tasks\n",
        "- we'll use the text-generation pipeline"
      ],
      "metadata": {
        "id": "ELg26tx5UQ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the previously downloaded model\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "pipe_input_batch = [\"Hey, how are you?\", \"I'm fine thanks. What do you think about communism?\"]\n",
        "output_batch = generator(pipe_input_batch)\n",
        "print(output_batch[1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIiJNgwUUPT9",
        "outputId": "0f062be6-3042-4bfa-a96a-591215dfe386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"I'm fine thanks. What do you think about communism? A system where the state owns everything\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the GPU, otherwise the cache will grow and eventually take up most of the VRAM\n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "VGyNB24HDpaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access the model's hidden states\n"
      ],
      "metadata": {
        "id": "5lMMxeIAJjYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "class HiddenStateExtractor:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        # Store activation hooks and hidden states\n",
        "        self.hooks = []\n",
        "        self.hidden_states = {}\n",
        "\n",
        "    def _get_activation(self, name):\n",
        "        \"\"\"Create a hook function to capture layer activations.\"\"\"\n",
        "        def hook(model, input, output):\n",
        "            # For attention layers, output is typically a tuple\n",
        "            if isinstance(output, tuple):\n",
        "                self.hidden_states[name] = output[0].detach()  # Get attention output\n",
        "            else:\n",
        "                self.hidden_states[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    def attach_hooks(self):\n",
        "        \"\"\"Attach hooks to all transformer layers.\"\"\"\n",
        "        # Clear existing hooks\n",
        "        self.remove_hooks()\n",
        "\n",
        "        # Attach new hooks to each layer\n",
        "        for name, module in self.model.named_modules():\n",
        "            # Capture self-attention output\n",
        "            if \"self_attn\" in name:\n",
        "                hook = module.register_forward_hook(self._get_activation(f\"{name}_attention\"))\n",
        "                self.hooks.append(hook)\n",
        "            # Capture MLP output\n",
        "            elif \"mlp\" in name:\n",
        "                hook = module.register_forward_hook(self._get_activation(f\"{name}_mlp\"))\n",
        "                self.hooks.append(hook)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove all hooks.\"\"\"\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n",
        "        self.hidden_states = {}\n",
        "\n",
        "    def get_hidden_states(self, input_text, get_last_token=True):\n",
        "        \"\"\"\n",
        "        Get hidden states for input text.\n",
        "\n",
        "        Args:\n",
        "            input_text (str): Input text to process\n",
        "            get_last_token (bool): If True, return only the last token's hidden states\n",
        "\n",
        "        Returns:\n",
        "            dict: Hidden states from different layers\n",
        "        \"\"\"\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Attach hooks\n",
        "        self.attach_hooks()\n",
        "\n",
        "        # Forward pass with output_hidden_states=True\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                **inputs,\n",
        "                output_hidden_states=True,\n",
        "                output_attentions=True,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # For CausalLM, hidden states are in outputs.hidden_states\n",
        "        all_hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Process hidden states\n",
        "        processed_states = {\n",
        "            \"all_layer_states\": [\n",
        "                state.detach().cpu() for state in all_hidden_states\n",
        "            ],\n",
        "            \"layer_activations\": self.hidden_states,\n",
        "            \"logits\": outputs.logits.detach().cpu(),  # Instead of last_hidden_state\n",
        "            \"attentions\": [attn.detach().cpu() for attn in outputs.attentions] if hasattr(outputs, 'attentions') and outputs.attentions else None\n",
        "        }\n",
        "\n",
        "        # If only last token is requested\n",
        "        if get_last_token:\n",
        "            processed_states = {\n",
        "                k: (v[-1, -1] if isinstance(v, torch.Tensor) else\n",
        "                    [state[:, -1] for state in v] if isinstance(v, list) else\n",
        "                    {layer: state[:, -1] for layer, state in v.items()})\n",
        "                for k, v in processed_states.items()\n",
        "            }\n",
        "\n",
        "        # Remove hooks\n",
        "        self.remove_hooks()\n",
        "\n",
        "        return processed_states\n",
        "\n",
        "    def analyze_hidden_states(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Analyze hidden states statistics.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (dict): Hidden states from get_hidden_states\n",
        "\n",
        "        Returns:\n",
        "            dict: Statistics about the hidden states\n",
        "        \"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        # Analyze layer-wise statistics\n",
        "        if \"all_layer_states\" in hidden_states:\n",
        "            layer_stats = []\n",
        "            for i, layer_state in enumerate(hidden_states[\"all_layer_states\"]):\n",
        "                layer_stats.append({\n",
        "                    \"layer\": i,\n",
        "                    \"mean\": layer_state.mean().item(),\n",
        "                    \"std\": layer_state.std().item(),\n",
        "                    \"min\": layer_state.min().item(),\n",
        "                    \"max\": layer_state.max().item(),\n",
        "                    \"norm\": torch.norm(layer_state).item()\n",
        "                })\n",
        "            stats[\"layer_statistics\"] = layer_stats\n",
        "\n",
        "        # Analyze attention patterns if available\n",
        "        if hidden_states[\"attentions\"]:\n",
        "            attention_stats = []\n",
        "            for i, attn in enumerate(hidden_states[\"attentions\"]):\n",
        "                attention_stats.append({\n",
        "                    \"layer\": i,\n",
        "                    \"attention_mean\": attn.mean().item(),\n",
        "                    \"attention_std\": attn.std().item(),\n",
        "                    \"max_attention\": attn.max().item()\n",
        "                })\n",
        "            stats[\"attention_statistics\"] = attention_stats\n",
        "\n",
        "        return stats\n",
        "\n",
        "# Example usage\n",
        "def demonstrate_hidden_states_extraction(model, tokenizer, input_text):\n",
        "    \"\"\"\n",
        "    Demonstrate how to extract and analyze hidden states.\n",
        "    \"\"\"\n",
        "    extractor = HiddenStateExtractor(model, tokenizer)\n",
        "\n",
        "    # Get hidden states for last token\n",
        "    hidden_states = extractor.get_hidden_states(input_text, get_last_token=True)\n",
        "\n",
        "    # Analyze the states\n",
        "    stats = extractor.analyze_hidden_states(hidden_states)\n",
        "\n",
        "    return hidden_states, stats\n",
        "\n",
        "# Usage example:\n",
        "input_text = \"Hello, how are you?\"\n",
        "hidden_states, stats = demonstrate_hidden_states_extraction(model, tokenizer, input_text)\n",
        "\n",
        "# Access final layer representation (from all_layer_states)\n",
        "final_layer_state = hidden_states[\"all_layer_states\"][-1]\n",
        "\n",
        "# Access logits\n",
        "logits = hidden_states[\"logits\"]\n",
        "\n",
        "# Access specific layer activations\n",
        "layer_activations = hidden_states[\"layer_activations\"]"
      ],
      "metadata": {
        "id": "Ji6VepqjeG3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Hidden states - Attention blocks: {len(hidden_states[\"attentions\"])}')\n",
        "print(f'Hidden states - Feed-forward layers: {len(hidden_states[\"all_layer_states\"])}')\n",
        "print(f'Last layer representation shape: {final_layer_state.shape}')\n",
        "print(f'Probability of possible output tokens (logits) shape: {logits.shape}')\n",
        "print(f'Feed-forward Layer statistics: {len(stats[\"layer_statistics\"])}')\n",
        "print(f'-----Attention head example----')\n",
        "print(f'Attention head shape: {hidden_states[\"attentions\"][0].shape}')\n",
        "print(f'-----Layer statistics example----')\n",
        "print(f'Layer statistics: Layer {stats[\"layer_statistics\"][5][\"layer\"]}')\n",
        "for key, value in stats[\"layer_statistics\"][0].items():\n",
        "    if key != \"layer\":\n",
        "      print(f'{key}: {value}')\n",
        "# decode the 10 most-likely logits to tokens\n",
        "most_likely_top_10 = logits.topk(10)\n",
        "print(f'-----Most likely tokens----')\n",
        "for i in range(10):\n",
        "  print(f'{i+1}th most likely token: {tokenizer.decode(most_likely_top_10.indices[i])}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbOs3l6uitLv",
        "outputId": "627fd49e-11ee-41f8-84c1-bccbf4af4301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden states - Attention blocks: 16\n",
            "Hidden states - Feed-forward layers: 17\n",
            "Last layer representation shape: torch.Size([1, 2048])\n",
            "Probability of possible output tokens (logits) shape: torch.Size([128256])\n",
            "Feed-forward Layer statistics: 17\n",
            "-----Attention head example----\n",
            "Attention head shape: torch.Size([1, 7, 7])\n",
            "-----Layer statistics example----\n",
            "Layer statistics: Layer 5\n",
            "mean: 1.1272626579739153e-05\n",
            "std: 0.01720438525080681\n",
            "min: -0.06201171875\n",
            "max: 0.06982421875\n",
            "norm: 0.7783914804458618\n",
            "-----Most likely tokens----\n",
            "1th most likely token:  I\n",
            "2th most likely token:  Today\n",
            "3th most likely token:  \n",
            "\n",
            "\n",
            "4th most likely token:  It\n",
            "5th most likely token:  What\n",
            "6th most likely token: <|eot_id|>\n",
            "7th most likely token:  Do\n",
            "8th most likely token:  Is\n",
            "9th most likely token:  Are\n",
            "10th most likely token:  My\n"
          ]
        }
      ]
    }
  ]
}